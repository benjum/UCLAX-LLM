{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50107253-31d6-42b2-b06f-36d795d7b359",
   "metadata": {},
   "source": [
    "# Intro to using PyTorch for a simple LLM\n",
    "\n",
    "This notebook is aimed very narrowly at deciphering transformer-relevant code.\n",
    "\n",
    "* Tensors (PyTorch’s main data structure)\n",
    "* Shapes and dimensions\n",
    "* `nn.Module` and `forward`\n",
    "* Common layers: `Embedding`, `Linear`, `Softmax`\n",
    "* Combining these elements into a toy mini-network\n",
    "\n",
    "We will review more PyTorch examples in future weeks, and give some additional examples of more basic neural networks.  For now, we are laying out the key ideas for transformers.\n",
    "\n",
    "If you want to get a better grasp of PyTorch, as well as see a more thorough introduction to building an LLM from Scratch, see:\n",
    "* https://github.com/rasbt/LLMs-from-scratch : the repo for the book \"Build a Large Language Model (From Scratch)\"\n",
    "* Appendix A of that repo has a PyTorch intro, for example: https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-A/01_main-chapter-code/code-part1.ipynb\n",
    "* Andrej Karpathy also has some excellent videos, including this one: [\"Let's build GPT: from scratch, in code, spelled out.\"](https://youtu.be/kCc8FmEb1nY?si=v5wAKd8b83EzstyZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8237aaa-dab4-4960-ba21-396c9519568c",
   "metadata": {},
   "source": [
    "## What is PyTorch?\n",
    "\n",
    "PyTorch is a numerical computing library (like NumPy) + autograd (automatic differentiation) + neural network utilities.\n",
    "\n",
    "Core idea: you work with tensors (multidimensional arrays) and modules (layers/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34cd8e3-92af-458d-ae63-bd94ec8ce09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa69719-7015-4ac7-ad6d-529264d68ada",
   "metadata": {},
   "source": [
    "## Tensors: PyTorch’s arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37381311-acf4-4bf1-b295-8d4c352aff14",
   "metadata": {},
   "source": [
    "### Creating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac909bc6-181b-49a2-90b4-b30eaea82be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar\n",
    "a = torch.tensor(3.0)\n",
    "\n",
    "# 1D tensor (vector)\n",
    "v = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# 2D tensor (matrix)\n",
    "M = torch.tensor([[1.0, 2.0],\n",
    "                  [3.0, 4.0]])\n",
    "\n",
    "print(\"a:\", a)\n",
    "print(\"v:\", v)\n",
    "print(\"M:\", M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c6581e-375e-49d5-918f-dd176dbaa2f7",
   "metadata": {},
   "source": [
    "### Tensor shapes \n",
    "##### (`.shape`)\n",
    "\n",
    "Shape = dimensionality and how many elements each dimension has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a9d550-25f3-4434-a7f3-5ae9017fb64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a.shape:\", a.shape)  # ()\n",
    "print(\"v.shape:\", v.shape)  # (3,)\n",
    "print(\"M.shape:\", M.shape)  # (2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dea42b-7b5a-4541-98bc-4495e9902c09",
   "metadata": {},
   "source": [
    "In the transformer code, we'll see 2D and 3D tensors, something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b297f-04b8-43f0-83e5-c86805d0bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2     # number of text sequences\n",
    "seq_len = 5        # number of words (tokens) in the sequence\n",
    "emb_len = 16       # size of the embedding or language model vector\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, emb_len)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c70d993-da10-4644-b1db-d4d7c90dcdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9543b9c-907f-43a5-baf7-7cd6c0574aa4",
   "metadata": {},
   "source": [
    "## Basic operations\n",
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c0529-670b-4999-b8c6-b76fda311e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[10, 20, 30],\n",
    "                  [40, 50, 60]])\n",
    "\n",
    "print(x[0])      # first row\n",
    "print(x[1, 2])   # row with index 1, col with index 2\n",
    "                 # i.e. second row, third column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a5a4a-1a3d-4095-b093-63279995f6c2",
   "metadata": {},
   "source": [
    "### Simple math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecae476-a473-4f69-abcf-9539ed31ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([10.0, 20.0, 30.0])\n",
    "\n",
    "print(a + b)          # elementwise add\n",
    "print(a * b)          # elementwise multiply\n",
    "print(a @ b)          # matrix multiplication and dot product operations between tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2dd640-a4e4-471d-9b24-a1831d3380e4",
   "metadata": {},
   "source": [
    "For neural networks, we use lots of matrix multiplications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67142563-f5a9-4639-bb19-03c7d4c475cd",
   "metadata": {},
   "source": [
    "<img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/dynamic_resizing_neural_network_4_obs.png\" alt=\"NNMatrixMultiply\" width=\"500\">\n",
    "* https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f802c-d69b-41a4-9750-f24a33689514",
   "metadata": {},
   "source": [
    "## `nn.Module`: how PyTorch defines layers and models\n",
    "\n",
    "Every neural net \"thing\" (layer, model) is usually a subclass of `nn.Module`.\n",
    "* If you haven't used Python classes, a Python class is just a way to bundle data and behavior together.  It's like a custom blueprint for making objects.\n",
    "* If there was a Python class called \"Student\", then we might illustrate as:\n",
    "  * **Class**: a blueprint (e.g., `Student`).\n",
    "  * **Object / instance**: a thing made from the blueprint (e.g., `student1`).\n",
    "  * **Attribute**: a variable that belongs to an object (`student1.name`).\n",
    "  * **Method**: a function inside a class that uses the object’s data (`student1.average_grade()`).\n",
    "  * `self`: a reference to \"this object right here\".\n",
    "\n",
    "For classes with PyTorch that are subclasses of `nn.Module`:\n",
    "* `super()` lets us call code from `nn.Module` without hard-coding anything from `nn.Module`\n",
    "* `__init__` is used to set up layers\n",
    "  * This method is called anytime a new instance is initialized\n",
    "* `forward(self, x)` is used to describe the computations\n",
    "\n",
    "### A tiny linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74490d9-017b-4d34-8448-cb68f3f8ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TinyModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()  # important!\n",
    "\n",
    "        # Layers (parameters are created here)\n",
    "        # We'll get to this later, but nn.Linear is essentially implementing the matrix operations\n",
    "        # that we need, such as y = x @ W^T + b (a fully-connected layer)\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, input_dim)\n",
    "        h = self.linear1(x)       # h of size (batch, hidden_dim)\n",
    "        h = F.relu(h)             # activation function\n",
    "        out = self.linear2(h)     # out of size (batch, output_dim)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8cd65d-706c-4588-a2ae-661609a8572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 4)  # 2 data records, each of dimension 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd38788-d24e-401a-a242-e4b458603356",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f12f9-0b3d-4fee-8b6d-1518aae80461",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyModel(input_dim=4, hidden_dim=8, output_dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21122a-0e8c-49e0-a5f4-b69c1b63702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a0891-2975-4513-873b-01938eed2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits.shape)    # should be model output for 2 records with 3 values each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c0ff5-3ed9-436f-8939-846ba89a41db",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9b9bd-d9a4-4a42-9344-4971351d2618",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "print(\"logits:\\n\", logits)\n",
    "print(\"probs:\\n\", probs)\n",
    "print(\"sum of probs:\\n\", [i.sum() for i in probs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012403b3-17ce-4b3f-8460-e30c3edd8b5b",
   "metadata": {},
   "source": [
    "### Key points:\n",
    "\n",
    "* `TinyModel(...)` creates a model with parameters.\n",
    "* Calling `model(x)` automatically calls `forward`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7eca9-0927-48b9-adff-0c8fa2721162",
   "metadata": {},
   "source": [
    "## Layers for transformers\n",
    "### Embeddings\n",
    "##### `nn.Embedding`\n",
    "\n",
    "Maps integer IDs (tokens) to vectors, and these can be trainable during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9412f7df-c1e4-478e-8d2c-9e2d4ed57047",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "d_model = 32\n",
    "embedding = nn.Embedding(vocab_size, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d410a06-b959-4ef2-af01-d70faf80086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we have 2 sequences of length 5:\n",
    "input_ids = torch.randint(0, vocab_size, (2, 5))  # random token IDs\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff18467-b603-4df5-8b67-83299664ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input_ids.shape:\", input_ids.shape)        # (2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb6ac2c-9f0a-4600-b156-d96abf16165d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emb = embedding(input_ids)\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846921e-ea30-45db-b5d2-e5e798236e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"emb.shape:\", emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db44c41-f896-42ae-bb6d-b96e2d767cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[10, 20, 30],\n",
    "                  [10, 50, 60]])\n",
    "emb = embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f9dbf-a1dc-4efa-b138-85fbfe75150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,0], emb[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c8d4a6-8f5f-487c-8162-d870e1aacaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1,0], emb[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3ab05-a8c0-4893-b908-e9716981024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1,1], emb[1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027f15ed-6028-4e33-b53c-575c69306fa2",
   "metadata": {},
   "source": [
    "#### Interpretation:\n",
    "\n",
    "* Each integer ID -> row in an embedding matrix -> vector of size d_model.\n",
    "* After embedding, we have continuous vectors that go into transformer layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e7b492-bcf8-4cc9-9ebe-126741f7eb19",
   "metadata": {},
   "source": [
    "### Linear layers\n",
    "##### `nn.Linear`\n",
    "\n",
    "Implements: y = x @ W^T + b (a fully-connected layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c852d4-c83f-49b7-9ce7-aa0b0cb4d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(in_features=32, out_features=10)\n",
    "\n",
    "x = torch.randn(2, 5, 32)          # (batch, seq, d_model)\n",
    "out = linear(x)                    # (batch, seq, 10)\n",
    "print(\"out.shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df64e98-3a5c-4743-8eed-d11c3489af2f",
   "metadata": {},
   "source": [
    "For LLMs, last layer often is `nn.Linear(d_model, vocab_size)`\n",
    "* takes hidden state and returns logits over the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d78cb30-5032-4afa-8718-721dc12e2b2f",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "##### `F.softmax`\n",
    "\n",
    "Turns logits (real numbers) into probabilities that sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e953a00-0555-4e67-af32-770b248b0fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "print(\"logits:\", logits)\n",
    "print(\"probs:\", probs)\n",
    "print(\"sum of probs:\", probs.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a4e866-0175-4543-9383-7fc52ebd39b0",
   "metadata": {},
   "source": [
    "## A tiny end-to-end example (like a micro language model)\n",
    "\n",
    "This mimics a transformer in a tiny, simple network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93edc0a1-f6c5-463f-8e25-bef2378e2801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TinyToyLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: (batch, seq_len)\n",
    "        x = self.embed(input_ids)          # (batch, seq_len, d_model)\n",
    "\n",
    "        # Just use the last token's representation (like a very dumb LM)\n",
    "        last_hidden = x[:, -1, :]          # (batch, d_model)\n",
    "\n",
    "        logits = self.linear(last_hidden)  # (batch, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dbb53f-2091-4691-add6-cf806804ffd4",
   "metadata": {},
   "source": [
    "**Example usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b642cc2e-1788-40e5-818a-eea958903353",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20\n",
    "d_model = 16\n",
    "model = TinyToyLM(vocab_size, d_model)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "# set up dummy tensor with random IDs ranging from 0 to vocab_size\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "logits = model(input_ids)\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "print(\"input_ids:\\n\", input_ids)\n",
    "print(\"logits shape:\", logits.shape)\n",
    "print(\"probs[0] sums to:\", probs[0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d392ed6-a0b2-4c91-aa94-4a892ce1bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a42135-095b-4de4-b1e2-b1c41d1360cf",
   "metadata": {},
   "source": [
    "### Core pattern:\n",
    "\n",
    "* Integers in (input_ids)\n",
    "* Embeddings (via nn.Embedding)\n",
    "* Some computation (here trivial, in transformers it’s self-attention layers)\n",
    "* Linear layer to logits\n",
    "* Optional softmax to get probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e55145-0318-4491-b55f-25e2ea668416",
   "metadata": {},
   "source": [
    "## How this maps to transformer examples\n",
    "\n",
    "When you see something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d17596-d018-452e-90da-3701836e873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([...])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.out_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b37890-78a9-4637-a36e-4f1a27aa659e",
   "metadata": {},
   "source": [
    "We can interpret this as:\n",
    "* `input_ids` is just a tensor of integers (token IDs).\n",
    "* `self.embed` is like the simple nn.Embedding example above.\n",
    "* `self.layers` is a list of more complex blocks (self-attention + feedforward).\n",
    "* `self.out_head` is a Linear mapping from d_model -> vocab_size.\n",
    "* The output logits can be turned into probabilities with softmax."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
