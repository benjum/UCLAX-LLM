{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ee1a19-1b5b-4131-acd8-3a97568e1ed6",
   "metadata": {},
   "source": [
    "# Training our mini-transformer to predict words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e88288-c42f-489f-81b1-2cd086921691",
   "metadata": {},
   "source": [
    "We are going to keep building on our TinyLM class to get more explicit about the transformer pieces.\n",
    "\n",
    "But...!  We also need to look at how we train it.  How do we changes the initial weights so they do a good job at generating word sequences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a67952-737c-4d4e-bf68-fd2901fb92bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec15db-8a40-432f-8ffd-ec0d6c7f9e75",
   "metadata": {},
   "source": [
    "Here we create a super simple corpus and initialize a vocab dictionary with some useful tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4dd9a-3977-4c2b-8b84-2e02d83a765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny toy corpus\n",
    "corpus = [\n",
    "    \"hello , how are you ?\",\n",
    "    \"hello , how is your day ?\",\n",
    "    \"how are you ?\",\n",
    "    \"how is your day ?\",\n",
    "]\n",
    "\n",
    "vocab = {\"<pad>\": 0, \n",
    "         \"<bos>\": 1, \n",
    "         \"<eos>\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b321db09-79fc-4c08-b530-f4401c691a09",
   "metadata": {},
   "source": [
    "We need to get the tokens from our corpus and add them to the vocab (i.e. establish their token IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442e0cf0-9b1c-4595-a603-3058f605b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = set()\n",
    "for i in corpus:\n",
    "    for word in i.split():\n",
    "        tokens.add(word)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fdee2c-b852-402d-b550-961255b616ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix,val in enumerate(tokens):\n",
    "    vocab[val] = ix+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38c502-7c1d-41bf-a92c-bcd314fa51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21179ccc-bd73-4e46-a214-e5c4e5fc5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the reverse correspondence too:\n",
    "id2token = {i: t for t, i in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd355dc-279a-4216-9b54-830e61e31eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c0291-6a3b-4b16-8858-0021d5681d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    return [vocab[\"<bos>\"]] + [vocab[w] for w in text.split()] + [vocab[\"<eos>\"]]\n",
    "\n",
    "def detokenize(ids):\n",
    "    return \" \".join(id2token[i] for i in ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb8d8c-9203-41d7-a4ed-5e92095a8f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will give an error!\n",
    "simple_tokenize('Hi my name is Ben')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd4c000-2087-4db5-bf48-7ad6b7088176",
   "metadata": {},
   "source": [
    "This is a problem we run into with text/tokens that are OOV -- Out of Vocabulary.\n",
    "\n",
    "Here we ignore it and just use words that are in our vocabulary, but sub-word tokenization helps to alleviate this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ef00a-e832-4c09-85c0-c8ad0b64db3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tokenize('how is your day ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8b6cdb-d46f-475a-9fdc-bf9474bfabef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_ids = simple_tokenize('how is your day ?')\n",
    "detokenize(tk_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b11308-29c5-4efa-a914-8db1301be9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, \n",
    "                nhead=2, \n",
    "                dim_feedforward=64, \n",
    "                batch_first=True\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.out_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: (batch, seq_len)\n",
    "        x = self.embed(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.out_head(x)  # (batch, seq_len, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a67d4-43a9-4baa-8c6b-15bd120b8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an instance of our model\n",
    "vocab_size = len(vocab)\n",
    "model = TinyLM(vocab_size=vocab_size, d_model=32, n_layers=2)\n",
    "\n",
    "# Convert a sample input text to numerical IDs\n",
    "text = \"hello , how are you ?\"\n",
    "token_ids = torch.tensor([simple_tokenize(text)])  # shape: (1, seq_len)\n",
    "\n",
    "# Get probable next word from our model\n",
    "with torch.no_grad():\n",
    "    logits = model(token_ids)                      # (1, seq_len, vocab_size)\n",
    "    next_token_logits = logits[0, -1]              # last position\n",
    "    probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "# Print out input text, input IDs, and distribution of probable next words\n",
    "print(\"Input text: \", text)\n",
    "print(\"Token IDs:  \", token_ids.tolist())\n",
    "print(\"Next-token distribution (top 5):\")\n",
    "top_probs, top_ids = probs.topk(5)\n",
    "for p, i in zip(top_probs, top_ids):\n",
    "    print(f\"  {id2token[i.item()]:>6s}: {p.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c8e771-cb7e-4937-af9e-a3834419560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt_ids, max_new_tokens=10):\n",
    "    \"\"\"\n",
    "    Greedy generation (argmax) with a simple loop.\n",
    "    prompt_ids: LongTensor of shape (1, seq_len)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated = prompt_ids.clone()\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated)          # (1, cur_len, vocab_size)\n",
    "            next_token_logits = logits[0, -1]  # (vocab_size,)\n",
    "\n",
    "            # turn into probabilities (not strictly needed for argmax, but fine)\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "            # greedy: pick the most likely token\n",
    "            next_token_id = torch.argmax(probs)  # scalar tensor\n",
    "\n",
    "        # append new token\n",
    "        generated = torch.cat(\n",
    "            [generated, next_token_id.view(1, 1)], dim=1\n",
    "        )\n",
    "\n",
    "        # stop if we hit <eos>\n",
    "        if next_token_id.item() == vocab[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    return generated  # (1, new_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cead0149-9397-4115-927b-63a328b59d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input\n",
    "text = \"hello , how are you ?\"\n",
    "full_ids = simple_tokenize(text)           # [<bos>, hello, ',', how, are, you, '?', <eos>]\n",
    "\n",
    "# Drop the final <eos> so the model can generate its own ending\n",
    "prompt_ids = torch.tensor([full_ids[:-1]]) # shape: (1, seq_len_without_eos)\n",
    "\n",
    "generated_ids = generate(model, prompt_ids, max_new_tokens=15)\n",
    "generated_ids_list = generated_ids[0].tolist()\n",
    "\n",
    "print(\"Prompt text:    \", detokenize(prompt_ids[0].tolist()))\n",
    "print(\"Generated IDs:  \", generated_ids_list)\n",
    "print(\"Generated text: \", detokenize(generated_ids_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f912b91-16bc-43f2-a91c-a3bef37cba30",
   "metadata": {},
   "source": [
    "### Make the model causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc666a-1d02-4826-a41e-9ac786d13aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=2,\n",
    "                dim_feedforward=64,\n",
    "                batch_first=True,\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.out_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: (batch, seq_len)\n",
    "        x = self.embed(input_ids)  # (batch, seq_len, d_model)\n",
    "\n",
    "        seq_len = x.size(1)\n",
    "        # causal mask: (seq_len, seq_len)\n",
    "        # mask[i, j] = -inf if j > i (can't attend to future)\n",
    "        mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len) * float(\"-inf\"),\n",
    "            diagonal=1\n",
    "        )\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask=mask)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.out_head(x)  # (batch, seq_len, vocab_size)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f8502-d3a2-481c-97f6-ecfa1f851a75",
   "metadata": {},
   "source": [
    "### Training!\n",
    "\n",
    "We need inputs and target outputs to compare against our model outputs.\n",
    "\n",
    "* Example input sequence: \"\\<bos\\> hello , how are you ?\"\n",
    "* Example target sequence: \"hello , how are you ? \\<eos\\>\"\n",
    "\n",
    "Each position predicts the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01e34f-70d7-48b5-96f3-4756fe227661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example(text):\n",
    "    ids = simple_tokenize(text)   # [<bos>, ..., <eos>]\n",
    "    input_ids = ids[:-1]          # drop last token\n",
    "    target_ids = ids[1:]          # drop first token\n",
    "    return input_ids, target_ids\n",
    "\n",
    "train_inputs = []\n",
    "train_targets = []\n",
    "\n",
    "for text in corpus:\n",
    "    inp, tgt = make_example(text)\n",
    "    train_inputs.append(inp)\n",
    "    train_targets.append(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa092a3-695f-4505-a570-32fced3ff3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenize(train_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc1cca-1f65-474b-b5a5-a2e4a3621e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenize(train_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7221e2bf-3dd5-4e3b-9c0c-834ab4fbac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a single batch by padding to same length\n",
    "max_len = max(len(x) for x in train_inputs)\n",
    "\n",
    "def pad(seq, max_len, pad_id=0):\n",
    "    return seq + [pad_id] * (max_len - len(seq))\n",
    "\n",
    "input_batch = torch.tensor([pad(x, max_len) for x in train_inputs])   # (batch, seq_len)\n",
    "target_batch = torch.tensor([pad(y, max_len) for y in train_targets]) # (batch, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a6313-4375-4e42-a4b5-0d44b7133418",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a8cfd8-c740-408e-a793-6b40c4c8dac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenize([i for i in input_batch[2].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475cda4c-f673-4b6e-b736-cb3a2d5a90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenize([i for i in target_batch[2].numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f86881b-9abc-4632-bc03-2594c67a762b",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Use CrossEntropyLoss over the vocabulary at every position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b84ef-e856-4b4d-b821-3cf317ef77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyLM(vocab_size=vocab_size, d_model=32, n_layers=2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()          # for token prediction\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 200  # small corpus, will overfit fast\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward\n",
    "    logits = model(input_batch)  # (batch, seq_len, vocab_size)\n",
    "\n",
    "    # reshape for CrossEntropyLoss: (batch * seq_len, vocab_size)\n",
    "    logits_flat = logits.view(-1, vocab_size)\n",
    "    targets_flat = target_batch.view(-1)   # (batch * seq_len,)\n",
    "\n",
    "    loss = criterion(logits_flat, targets_flat)\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11270ff6-b1cf-41c1-95cc-7c3ae41535d1",
   "metadata": {},
   "source": [
    "Reuse the greedy generate loop from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38d6ff-f0f0-470e-8b73-89bb6e01a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"how you hello\"\n",
    "ids = simple_tokenize(text)\n",
    "prompt_ids = torch.tensor([ids[:-1]])  # drop <eos>\n",
    "\n",
    "generated_ids = generate(model, prompt_ids, max_new_tokens=15)\n",
    "print(detokenize(generated_ids[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e663e6-892c-4538-bf8f-16e5c6d8bcb5",
   "metadata": {},
   "source": [
    "### What is the model structure?  How many weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7898eccf-87c1-411e-a679-63388564778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd092c-2f58-44ca-93ff-61a1604e31ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4aacb6-21d5-4971-a7b9-df97e84e2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "12*32 + 2*(4*33*32 + 33*64 + 65*32 + 2*32 + 2*32) + 2*32 + 33*12"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
