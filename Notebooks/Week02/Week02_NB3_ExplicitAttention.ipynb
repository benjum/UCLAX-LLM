{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ee1a19-1b5b-4131-acd8-3a97568e1ed6",
   "metadata": {},
   "source": [
    "# Including Attention, MLP, and Model Architecture Designs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2cb18f-5ad6-46ad-a3ab-a39471dc2a6a",
   "metadata": {},
   "source": [
    "Our current state of development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a67952-737c-4d4e-bf68-fd2901fb92bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4dd9a-3977-4c2b-8b84-2e02d83a765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny toy corpus\n",
    "corpus = [\n",
    "    \"hello , how are you ?\",\n",
    "    \"hello , how is your day ?\",\n",
    "    \"how are you ?\",\n",
    "    \"how is your day ?\",\n",
    "]\n",
    "\n",
    "vocab = {\"<pad>\": 0, \n",
    "         \"<bos>\": 1, \n",
    "         \"<eos>\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442e0cf0-9b1c-4595-a603-3058f605b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = set()\n",
    "for i in corpus:\n",
    "    for word in i.split():\n",
    "        tokens.add(word)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fdee2c-b852-402d-b550-961255b616ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix,val in enumerate(tokens):\n",
    "    vocab[val] = ix+3\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21179ccc-bd73-4e46-a214-e5c4e5fc5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the reverse correspondence too:\n",
    "id2token = {i: t for t, i in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c0291-6a3b-4b16-8858-0021d5681d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    return [vocab[\"<bos>\"]] + [vocab[w] for w in text.split()] + [vocab[\"<eos>\"]]\n",
    "\n",
    "def detokenize(ids):\n",
    "    return \" \".join(id2token[i] for i in ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2577ea-37e3-446c-83a9-d47668aea819",
   "metadata": {},
   "source": [
    "Our current model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc666a-1d02-4826-a41e-9ac786d13aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=2,\n",
    "                dim_feedforward=64,\n",
    "                batch_first=True,\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.out_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: (batch, seq_len)\n",
    "        x = self.embed(input_ids)  # (batch, seq_len, d_model)\n",
    "\n",
    "        seq_len = x.size(1)\n",
    "        # causal mask: (seq_len, seq_len)\n",
    "        # mask[i, j] = -inf if j > i (can't attend to future)\n",
    "        mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, device=x.device) * float(\"-inf\"),\n",
    "            diagonal=1\n",
    "        )\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask=mask)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.out_head(x)  # (batch, seq_len, vocab_size)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f86881b-9abc-4632-bc03-2594c67a762b",
   "metadata": {},
   "source": [
    "Our current training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b84ef-e856-4b4d-b821-3cf317ef77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TinyLM(vocab_size=vocab_size, d_model=32, n_layers=2)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()          # for token prediction\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# num_epochs = 200  # small corpus, will overfit fast\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # forward\n",
    "#     logits = model(input_batch)  # (batch, seq_len, vocab_size)\n",
    "\n",
    "#     # reshape for CrossEntropyLoss: (batch * seq_len, vocab_size)\n",
    "#     logits_flat = logits.view(-1, vocab_size)\n",
    "#     targets_flat = target_batch.view(-1)   # (batch * seq_len,)\n",
    "\n",
    "#     loss = criterion(logits_flat, targets_flat)\n",
    "\n",
    "#     # backward\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     if (epoch + 1) % 20 == 0:\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs} - loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7248894d-730e-4155-b0c5-d69b688a7b90",
   "metadata": {},
   "source": [
    "# .... Now .... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9621b55-b19a-43fb-ace5-c5e1b9a7b253",
   "metadata": {},
   "source": [
    "The following code above is our \"transformer block\" that has the attention and MLP:\n",
    ">            nn.TransformerEncoderLayer(\n",
    ">                d_model=d_model,\n",
    ">                nhead=2,\n",
    ">                dim_feedforward=64,\n",
    ">                batch_first=True,\n",
    ">            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1066bcbf-aa2a-4bdf-9676-b2aa46dac534",
   "metadata": {},
   "source": [
    "How can we replace this with more illuminating code?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5768e5-a8a1-4a31-98be-a9576d7e1de1",
   "metadata": {},
   "source": [
    "Let's first revise the TinyLM to accept a block that's more general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7015a840-2696-4ef9-b030-39d85d1d0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            # REPLACING:\n",
    "            # nn.TransformerEncoderLayer(\n",
    "            #     d_model=d_model,\n",
    "            #     nhead=2,\n",
    "            #     dim_feedforward=64,\n",
    "            #     batch_first=True,\n",
    "            # )\n",
    "            TinyAttnBlock(d_model, d_ff=64)   # our custom attention block\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.out_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: (batch, seq_len)\n",
    "        x = self.embed(input_ids)  # (batch, seq_len, d_model)\n",
    "\n",
    "        # ALSO REMOVE the MASK temporarily\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.out_head(x)  # (batch, seq_len, vocab_size)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d8d595-4dc9-4057-997c-1475afaec7f0",
   "metadata": {},
   "source": [
    "We need to define the appropriate TinyAttnBlock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9bf0ff-3a54-4d9f-95bf-fde41d7f4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyAttnBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=64):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = SelfAttention(d_model)  # SELF-ATTENTION\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(            # MLP\n",
    "            nn.Linear(d_model, d_ff),       # \n",
    "            nn.ReLU(),                      #\n",
    "            nn.Linear(d_ff, d_model),       #\n",
    "        )                                   #\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self-attention + residual\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        # feedforward + residual\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c71adc-d8b1-44dd-b6ea-da48af4131a6",
   "metadata": {},
   "source": [
    "And we can be explicit about the SelfAttention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ac34b-e003-42eb-8e62-8cd7cb5677ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        # separate projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        q = self.W_q(x)  # (B, T, C)\n",
    "        k = self.W_k(x)  # (B, T, C)\n",
    "        v = self.W_v(x)  # (B, T, C)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(C)  # (B, T, T)\n",
    "\n",
    "        # causal mask: no attending to future positions\n",
    "        mask = torch.triu(\n",
    "            torch.ones(T, T, device=x.device) * float(\"-inf\"),\n",
    "            diagonal=1\n",
    "        )\n",
    "        scores = scores + mask  # broadcast over batch\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)       # (B, T, T)\n",
    "        y = attn @ v                           # (B, T, C)\n",
    "\n",
    "        y = self.out(y)                        # (B, T, C)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96306f9-a5aa-4e75-9393-72fda020f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyLM(vocab_size=vocab_size, d_model=32, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8bf5ad-ac8d-44b7-89b9-33d5c9504e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8e70a-3cb4-4aec-a353-bc08ab6e1516",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8a03bd-e999-46e9-b958-05fd90b9d144",
   "metadata": {},
   "source": [
    "This is enough for us to make some key comments about different model architectures:\n",
    "* **Encoder-only models (BERT-style)**\n",
    "  * tokens can see each other in both directions; you usually only mask out padding\n",
    "  * in our SelfAttention: no causal mask (don't block out the \"future\")\n",
    "* **Decoder-only models (GPT-style)**\n",
    "  * tokens can only see past and current positions -> causal mask + optional padding mask\n",
    "  * in our SelfAttention: use the causal mask (`-inf` in upper triangle)\n",
    "* **Encoder-decoder models (T5-style)**\n",
    "  * Encoder blocks are the same as encoder-only model\n",
    "  * Decoder blocks have:\n",
    "     * Causal self-attention, just like the decoder-only model\n",
    "     * Cross-attention (attention decoder pays to the encoder output)\n",
    "       * Q comes from the decoder hidden states\n",
    "       * K and V come from the encoder outputs\n",
    "       * No causal mask (encoder tokens are not ordered relative to decoder tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa7e693-3f66-48cc-8d2a-cb0c3aa44473",
   "metadata": {},
   "source": [
    "### Encoder self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6041a-d8e3-4c55-8ae7-968e69fad87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x: (B, T, C)\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        q = self.W_q(x)  # (B, T, C)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(C)  # (B, T, T)\n",
    "\n",
    "        # KEY DIFFERENCE\n",
    "        # remove the causal mask\n",
    "        \n",
    "        # attn_mask: (B, 1, T) or (B, T, T), with 0 for keep, -inf for mask\n",
    "        if attn_mask is not None:\n",
    "            scores = scores + attn_mask\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)  # (B, T, T)\n",
    "        y = attn @ v                      # (B, T, C)\n",
    "        y = self.out(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad301a6c-a3e3-44bf-8523-de4bf624033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=64):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.self_attn = EncoderSelfAttention(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        x = x + self.self_attn(self.ln1(x), attn_mask=attn_mask)\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6c539-9778-413e-ac54-cbf6ab7dd56f",
   "metadata": {},
   "source": [
    "## Decoder self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cea18cd-e09c-458d-b872-79d358b5dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x: (B, T, C)\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(C)  # (B, T, T)\n",
    "\n",
    "        # KEY DIFFERENCE\n",
    "        # include the causal mask\n",
    "        # causal mask: prevent attending to j > i\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones(T, T, device=x.device) * float(\"-inf\"),\n",
    "            diagonal=1\n",
    "        )  # (T, T)\n",
    "\n",
    "        scores = scores + causal_mask  # broadcast to (B, T, T)\n",
    "\n",
    "        # optional extra mask (e.g., padding), same idea as encoder\n",
    "        if attn_mask is not None:\n",
    "            scores = scores + attn_mask\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        y = torch.matmul(attn, v)\n",
    "        y = self.out(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bd1f26-df54-440a-84ba-68b966095e0a",
   "metadata": {},
   "source": [
    "## Decoder block with cross-attention included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97398c53-c2c9-4d76-b2e9-52a3d14503ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # from decoder state\n",
    "        self.W_k = nn.Linear(d_model, d_model)  # from encoder output\n",
    "        self.W_v = nn.Linear(d_model, d_model)  # from encoder output\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x_dec, x_enc, enc_attn_mask=None):\n",
    "        \"\"\"\n",
    "        x_dec: (B, T_dec, C)  decoder hidden states\n",
    "        x_enc: (B, T_enc, C) encoder outputs\n",
    "        enc_attn_mask: mask over encoder positions, shape (B, 1, T_enc) or (B, T_dec, T_enc)\n",
    "        \"\"\"\n",
    "        B, T_dec, C = x_dec.size()\n",
    "        _, T_enc, _ = x_enc.size()\n",
    "\n",
    "        q = self.W_q(x_dec)  # (B, T_dec, C)\n",
    "        k = self.W_k(x_enc)  # (B, T_enc, C)\n",
    "        v = self.W_v(x_enc)  # (B, T_enc, C)\n",
    "\n",
    "        # scores: (B, T_dec, T_enc)\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(C)\n",
    "\n",
    "        if enc_attn_mask is not None:\n",
    "            scores = scores + enc_attn_mask  # mask out encoder pads\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)        # (B, T_dec, T_enc)\n",
    "        y = attn @ v                            # (B, T_dec, C)\n",
    "        y = self.out(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f611463-c244-4e31-989b-35833ba9cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=64):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.self_attn = DecoderSelfAttention(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.cross_attn = CrossAttention(d_model)\n",
    "        self.ln3 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_dec, x_enc, self_mask=None, enc_mask=None):\n",
    "        # causal self-attention over decoder tokens\n",
    "        x_dec = x_dec + self.self_attn(self.ln1(x_dec), attn_mask=self_mask)\n",
    "\n",
    "        # cross-attention to encoder outputs\n",
    "        x_dec = x_dec + self.cross_attn(self.ln2(x_dec), x_enc, enc_attn_mask=enc_mask)\n",
    "\n",
    "        # feedforward\n",
    "        x_dec = x_dec + self.ff(self.ln3(x_dec))\n",
    "        return x_dec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b8c08-d964-43b9-ac24-214a4d7a5914",
   "metadata": {},
   "source": [
    "## Encoder-decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc38eadf-fdb8-450b-b7fd-ee017819d725",
   "metadata": {},
   "source": [
    "Use both of the above.  Here we skip showing only the attention blocks like the above, and instead show the full (mini) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e8a88-5f20-437d-a455-11e5ff443ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinySeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers):\n",
    "        super().__init__()\n",
    "        self.src_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.tgt_embed = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderBlock(d_model, d_ff=64)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderBlock(d_model, d_ff=64)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.enc_ln_f = nn.LayerNorm(d_model)\n",
    "        self.dec_ln_f = nn.LayerNorm(d_model)\n",
    "        self.out_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    # separate encode/decode helpers (nice for generation)\n",
    "    def encode(self, src_ids):\n",
    "        # src_ids: (B, T_src)\n",
    "        x = self.src_embed(src_ids)  # (B, T_src, C)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        x = self.enc_ln_f(x)\n",
    "        return x  # encoder outputs\n",
    "\n",
    "    def decode(self, tgt_ids, enc_out):\n",
    "        # tgt_ids: (B, T_tgt), enc_out: (B, T_src, C)\n",
    "        x = self.tgt_embed(tgt_ids)  # (B, T_tgt, C)\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, enc_out)\n",
    "        x = self.dec_ln_f(x)\n",
    "        logits = self.out_head(x)    # (B, T_tgt, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, src_ids, tgt_ids):\n",
    "        enc_out = self.encode(src_ids)\n",
    "        logits = self.decode(tgt_ids, enc_out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b338053f-5878-4539-b8d6-ad4b90672f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinySeq2Seq(vocab_size=vocab_size, d_model=32, n_layers=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
