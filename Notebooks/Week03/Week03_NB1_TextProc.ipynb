{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5508d0c",
   "metadata": {},
   "source": [
    "# How can we think of text as numbers for quantitative analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e55b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c6a64",
   "metadata": {},
   "source": [
    "## Bag-of-Words (BoW)\n",
    "\n",
    "BoW represents a document as a set of words without regard for word order.  Each word is assigned a unique index, and a document is represented as a vector whose values at the index for each word are the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66faa9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"The cat slept and then meowed.\", \n",
    "          \"The tiger slept and then roared.\", \n",
    "          \"The boy ran home and then the boy laughed.\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a9832-bf20-437e-8a1b-ac3337eab1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782a1dd",
   "metadata": {},
   "source": [
    "Even though we are using Scikit-Learn to do the CountVectoriz-ing, there is no reason that we couldn't manually do it ourselves too with a bit of Python.  It's just convenient to do it the Scikit-Learn way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6495a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4589cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X.toarray(), \n",
    "             columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8342f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as to compare against our corpus:\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c767b",
   "metadata": {},
   "source": [
    "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "TF-IDF extends BoW by accounting for the uniqueness of words in distinguishing between documents.  The word counts of BoW are weighted by words' relative rarity across the entire corpus.\n",
    "\n",
    "* Scikit-Learn's TF-IDF calculation is [described here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca2c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d091825",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_tfidf.toarray(), \n",
    "             columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609e0fcb",
   "metadata": {},
   "source": [
    "There are a lot of mathematical details that come in here for trying to get well behaved forms of TF-IDF, and it's actually a messy business trying to back this out from the word counts and frequencies.\n",
    "\n",
    "You can ignore the following if you want to, but here is how one would go directly from the matrix of counts to scikit-learn's version of the TFIDF measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbbb0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bow = pd.DataFrame(X.toarray(), \n",
    "             columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f832d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb39a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the term frequencies in each of the three documents\n",
    "(x_bow.T / x_bow.T.sum(axis=0)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d72b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of documents in which each word occurs\n",
    "(x_bow > 0).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = (x_bow.T / x_bow.T.sum(axis=0)).T\n",
    "\n",
    "# the +1 at the end is so that even words that occur across all docs\n",
    "# still have a non-zero TFIDF\n",
    "# the +1 in numerator and +1 in denominator are conveniences to\n",
    "# handle the otherwise division by 0 for words that have 0 counts\n",
    "idf = np.log((1+3) / (1+(x_bow > 0).sum(axis=0))) + 1\n",
    "\n",
    "tf * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0621906",
   "metadata": {},
   "source": [
    "... and then one has to do a cosine normalization (the squares of elements in the rows add up to 1).  This is convenient because one can then do an inner (dot) product of rows to get a cosine similarity measure that varies between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef0045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tf * idf\n",
    "tfidf = (tfidf.T / np.sqrt((tfidf.T * tfidf.T).sum(axis=0))).T\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5443d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(tfidf.loc[0], tfidf.loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76042c71-fe25-46db-a92e-305f5cc0352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity matrix for every pair of documents:\n",
    "np.matmul(tfidf, tfidf.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a416d7d",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Word embeddings represent words as dense vectors in a continuous vector space. Word2Vec, GloVe, or FastText are pre-trained word embedding models that can be used to help obtain word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_corpus, \n",
    "                 vector_size=2,\n",
    "                 min_count=1)\n",
    "\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddde7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e27b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3540fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_for_document = [word_vectors[word] for word in tokenized_corpus[0] if word in word_vectors.index_to_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e350a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_for_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32940d",
   "metadata": {},
   "source": [
    "The dense vectors can allow us to look for similarity scores, e.g., by looking at the inner (dot) product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962097a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(word_vectors['cat'], word_vectors['meowed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc00e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(word_vectors['cat'], word_vectors['tiger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(word_vectors['cat'], word_vectors['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d186d8e4",
   "metadata": {},
   "source": [
    "# Word embedding plotting example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1cb04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283401ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {word: model.wv[word] for word in word_vectors.index_to_key}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for word, wordvec in word_embeddings.items():\n",
    "  ax.scatter(wordvec[0], wordvec[1])\n",
    "  ax.annotate(word, (wordvec[0], wordvec[1]))\n",
    "\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.title(\"Word Embeddings in 2D Space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b5c892",
   "metadata": {},
   "source": [
    "In the above, the \"2\" dimensions may be reasonable for plotting, but it's a dramatic projection of a high-dimensional space into a lower dimensional space for visualization.\n",
    "\n",
    "When the texts become really large, the problem becomes even more dramatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ab353-7fd4-4582-bd22-9e767dc41437",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text of \"Moby Dick\"\n",
    "from nltk.corpus import gutenberg\n",
    "moby_dick_text = gutenberg.raw('melville-moby_dick.txt')\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(moby_dick_text)\n",
    "words = word_tokenize(moby_dick_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6691bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2598de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f63638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only uncomment this if you want lots of output\n",
    "# moby_dick_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4304c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[55:56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da3566",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea787214",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus[55:56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=tokenized_corpus, \n",
    "                 vector_size=100,\n",
    "                 min_count=1)\n",
    "\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f691de",
   "metadata": {},
   "source": [
    "The similarity score is the cosine between the vectors representing the word embeddings.  The full word-document matrix is 255028-dimensional, while the word-embedding is only 100-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de7e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(model.wv['woman'], \n",
    "       model.wv['man']) / (np.linalg.norm(model.wv['woman']) * \n",
    "                           np.linalg.norm(model.wv['man']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb537e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('sea', 'scarcity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ba80dc-e50d-4f54-8c0b-9fb303d778d8",
   "metadata": {},
   "source": [
    "## Word Embeddings with `sentence-transformers` and BERT\n",
    "\n",
    "So far, we've seen how to represent text using traditional methods (e.g. bag-of-words, TF-IDF), as well as how to use a static embedding model like GloVe.\n",
    "\n",
    "Unlike GloVe and FastText, which treat words in isolation and assign a single vector per word regardless of context, Sentence Transformers use transformer architectures (like BERT) and are fine-tuned on tasks such as semantic similarity using contrastive loss.  Modern NLP models use dense vector embeddings that capture semantic similarity: texts with similar meaning end up close together in vector space.\n",
    "\n",
    "We now consider:\n",
    "1. How to set up sentence-level embeddings with the `sentence-transformers` library.\n",
    "2. How to get BERT-based embeddings directly from Hugging Face `transformers`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673291d5-5213-4ab5-b2a5-9f8baa042197",
   "metadata": {},
   "source": [
    "## 1. Sentence Embeddings with `sentence-transformers`\n",
    "\n",
    "The [`sentence-transformers`](https://www.sbert.net/) library wraps a variety of pre-trained transformer models\n",
    "and makes it very easy to get sentence-level embeddings.\n",
    "\n",
    "Flow:\n",
    "- Input: a list of sentences / texts.\n",
    "- Output: a NumPy array or PyTorch tensor of shape `(num_sentences, embedding_dim)`.\n",
    "- Models like `\"all-MiniLM-L6-v2\"` are small, fast, and good general-purpose choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08938c6-5075-4c40-b2f4-aa68855e202d",
   "metadata": {},
   "source": [
    "First, load the library and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45230ab1-332e-4555-a582-57312b197145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ee51f-486b-488d-9947-64b8adaaf864",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sent_model = SentenceTransformer(sent_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9200ad6e-d9ae-4413-9f4a-b904fcf1c088",
   "metadata": {},
   "source": [
    "Encode our sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8133ff38-1ee9-47de-8c8b-91bd9b1cb0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"The cat slept and then meowed.\",\n",
    "             \"The tiger slept and then roared.\",\n",
    "             \"The boy ran home and then the boy laughed.\",\n",
    "             \"This sentence has positive sentiment.\",\n",
    "             \"This sentence has negative sentiment.\"\n",
    "             \n",
    "]\n",
    "\n",
    "sentence_embeddings = sent_model.encode(sentences, \n",
    "                                        convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca288f7-6081-41db-9d43-21eb7895b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bba1d0-e346-4551-afe8-60155e274a64",
   "metadata": {},
   "source": [
    "Example numbers in the first embedding vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826111fa-8b5e-4a3f-86cf-78bce8a1315d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sentence_embeddings[0][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e64faf-79ec-429b-a863-014a401ab1e6",
   "metadata": {},
   "source": [
    "### Measuring Similarity\n",
    "\n",
    "We can measure similarity between embeddings using cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803afa31-07ee-4867-8895-9f5455e9266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise cosine similarities\n",
    "cosine_sim_matrix = util.cos_sim(sentence_embeddings, sentence_embeddings)\n",
    "\n",
    "cosine_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99793dd-e6c8-40dd-b816-03b56eeaedc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings @ sentence_embeddings.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f7d9c-21a4-4a75-b0a9-6ab678f7d413",
   "metadata": {},
   "source": [
    "## 2. Word / Sentence Embeddings with BERT (Hugging Face `transformers`)\n",
    "\n",
    "`sentence-transformers` is great for ready-to-use sentence embeddings, but sometimes you may want to:\n",
    "\n",
    "- Use a specific BERT variant (e.g. `bert-base-uncased`, `distilbert-base-uncased`, domain-specific BERT).\n",
    "- Control how embeddings are constructed (e.g. average over tokens vs. use the `[CLS]` token).\n",
    "\n",
    "We can do this directly with the Hugging Face `transformers` library.\n",
    "\n",
    "Typical procedure for sentence embeddings with BERT:\n",
    "\n",
    "1. Tokenize text with a BERT tokenizer.\n",
    "2. Run the tokens through the BERT model to get hidden states.\n",
    "3. Aggregate token embeddings (e.g. mean pooling across tokens) to get a single vector per sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe365184-7034-4799-92ea-77f664a77422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1faebf-e665-4c3f-8760-5facc5777873",
   "metadata": {},
   "source": [
    "Load BERT tokenizer and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b936a395-ca93-495a-b02f-c35ab4612b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c4290e-9f93-47ae-a450-19deb8495458",
   "metadata": {},
   "source": [
    "Put model in evaluation mode (disables dropout etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc755c8-fe6e-4a90-956a-163ef40c29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebb6ee8-af63-434d-a1a5-97006bd6c9af",
   "metadata": {},
   "source": [
    "Example sentences (same as before for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc21927-6de3-4ff6-9cb6-e698d4a79538",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"The cat slept and then meowed.\",\n",
    "             \"The tiger slept and then roared.\",\n",
    "             \"The boy ran home and then the boy laughed.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10debbc-5633-4943-b15f-6f1fb63d358e",
   "metadata": {},
   "source": [
    "Tokenize with padding & truncation so all sequences have same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc55187-96f7-4ee6-bb8a-6a4560289ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer(\n",
    "    sentences,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"  # return PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55415f1-f462-4fda-9fd3-c4c864ef9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9661e4d1-9465-4ae5-9644-da9d469706ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in encoded.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ad399-ec2c-4ea2-9749-dc942f8d2489",
   "metadata": {},
   "source": [
    "### Getting Embeddings from BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a5094-cdcd-43f9-8a2c-50d9dd19f3da",
   "metadata": {},
   "source": [
    "Run BERT on our tokenized inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d44e0-d041-43c6-981b-4e249f2c8214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short note about \"**varname\"\n",
    "\n",
    "data = {'name': 'Alice', 'age': 30}\n",
    "\n",
    "def greet(name, age): \n",
    "    print(f\"Hi {name}, you're {age}.\")\n",
    "\n",
    "greet(**data)  # Equivalent to greet(name='Alice', age=30)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c3667-3680-45c8-b6cb-a413f0ceb46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = bert_model(**encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b7018a-e0da-45e6-9473-77b841f5cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bf146b-ce78-487e-81d2-517468d75888",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba023a9-d07d-4af5-86a2-1e199979a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['pooler_output'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5436e86b-2d3b-493d-9246-d4605e02d80a",
   "metadata": {},
   "source": [
    "Feeding the tokenized `encoded` into our BERT model outputs:\n",
    "\n",
    "- `last_hidden_state`: a tensor of shape `(batch_size, sequence_length, hidden_size)`\n",
    "- Optionally `pooler_output` (for some models) and/or hidden states from each layer.\n",
    "\n",
    "Two common strategies to get a single vector per sentence:\n",
    "\n",
    "1. [CLS] token embedding: use `last_hidden_state[:, 0, :]`.\n",
    "2. Mean pooling: average all token embeddings, masking out padding tokens.\n",
    "\n",
    "We'll implement mean pooling since it often works well in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcfd6bf-b7f2-4ae6-a5f2-e4fe8bef1763",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f51a9-ac7c-4d6d-a420-9c5f88dcf5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce73dd9-e3b8-445c-a783-6978f204d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand attention mask so it matches token_embeddings shape\n",
    "input_mask_expanded = encoded[\"attention_mask\"].unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "input_mask_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcaa40a-08ed-452c-a7ab-6dfac99bebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mask_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebbc8af-79a9-4e47-ae4b-0340dd0a9242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum embeddings along the sequence length dimension\n",
    "sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "    \n",
    "# Count of non-masked tokens\n",
    "sum_mask = input_mask_expanded.sum(dim=1) \n",
    "# to prevent div by 0: torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "\n",
    "# Return average\n",
    "bert_sentence_embeddings = sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a93ff-0bfa-4535-bc7d-e5cee58f48ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec94d0b-425c-4542-91a1-6347496e6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_sentence_embeddings[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050c846-7cf7-43bf-a035-fa17695b45f8",
   "metadata": {},
   "source": [
    "### Computing Similarity with BERT Embeddings\n",
    "\n",
    "Just like with `sentence-transformers`, we can compute cosine similarity between BERT-based embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da02b4e6-5407-4ee2-ac63-a388c45ca520",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_sentence_embeddings @ bert_sentence_embeddings.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7bd9ba-e719-4968-9100-3f5e048015f1",
   "metadata": {},
   "source": [
    "Common to normalize first to get values between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fddafb-660b-4930-92b1-b706f75de3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "# Normalize embeddings before cosine similarity (optional but common)\n",
    "normalized_embeddings = F.normalize(bert_sentence_embeddings, \n",
    "                                    p=2,   # exponent of the norm, here we take an L2 norm\n",
    "                                    dim=1)\n",
    "\n",
    "# Pairwise cosine similarity matrix\n",
    "normalized_embeddings @ normalized_embeddings.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5d934-3729-4801-b8c6-6bd875be13c9",
   "metadata": {},
   "source": [
    "## When to Use What?\n",
    "\n",
    "- `sentence-transformers`:\n",
    "  - High-quality sentence embeddings with minimal effort.\n",
    "  - Models are fine-tuned on similarity/search tasks.\n",
    "  - One line `.encode()` call, very convenient.\n",
    "- Raw BERT via `transformers`:\n",
    "  - Best when you need full control:\n",
    "    - Custom pooling strategies.\n",
    "    - Intermediate layers.\n",
    "    - Domain-specific fine-tuning.\n",
    "  - Requires more code but is very flexible.\n",
    "- Start with a good `sentence-transformers` model.\n",
    "- If needed, switch to a custom BERT (or other transformer) setup and fine-tune it on your own data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
