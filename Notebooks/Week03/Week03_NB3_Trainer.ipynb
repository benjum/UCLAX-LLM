{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5508d0c",
   "metadata": {},
   "source": [
    "# How can we think of text as numbers for quantitative analysis -- Continued!\n",
    "\n",
    "We can turn text into embeddings, but how can we fine-tune a transformer model on your own data?  What do we need to do to text + labels for fine-tuning with `transformers.Trainer`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d101b98a-103c-42b5-93ad-ff3d92fd1a5c",
   "metadata": {},
   "source": [
    "Hugging Face’s `Trainer` API expects your data in a `datasets.Dataset` (or a PyTorch/TF dataset).  \n",
    "\n",
    "We will:\n",
    "1. Take raw Python lists of texts (and labels).\n",
    "2. Turn them into a `Dataset`.\n",
    "3. Tokenize them for a given transformer model.\n",
    "4. Feed them into `Trainer` for training.\n",
    "\n",
    "We’ll use a simple text classification example, but the same pattern applies to many tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a95f79e-3cfb-4cf3-a5a0-ea34a5341f3a",
   "metadata": {},
   "source": [
    "### 1. Create a `Dataset` from Raw Python Lists\n",
    "\n",
    "Assume we have a list of short texts and corresponding labels:\n",
    "\n",
    "- `0` = negative\n",
    "- `1` = positive\n",
    "\n",
    "We'll create a `datasets.Dataset` from them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103e71b-efbc-4d09-b44f-ef73e30a4164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0996698-9d62-45cb-a4b4-230b93e8673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example raw data\n",
    "texts = [\n",
    "    \"I love this movie, it was fantastic!\",\n",
    "    \"The plot was boring and predictable.\",\n",
    "    \"Amazing performance by the lead actor.\",\n",
    "    \"I didn't enjoy the film at all.\",\n",
    "    \"It was okay, some parts were good.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce376e8-9d41-4939-af51-052d9225aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1, 0, 1, 0, 1]  # 1 = positive, 0 = negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc73f547-f71f-4125-9e7f-0e0f4fa042e2",
   "metadata": {},
   "source": [
    "Create a Dataset from a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965bb908-e421-422e-a26b-d17884405a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = Dataset.from_dict({\n",
    "    \"text\": texts,\n",
    "    \"label\": labels,\n",
    "})\n",
    "\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aacb37-f43c-4428-bcc6-c3328bbfcce3",
   "metadata": {},
   "source": [
    "For now we’ll pretend this small dataset is our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715dd12-0b56-4b9c-84fb-d2b4e6bd2c17",
   "metadata": {},
   "source": [
    "### 2. Tokenize the Dataset\n",
    "\n",
    "`Trainer` works with tokenized inputs. We’ll use BERT, but you can swap in any model name from the Hugging Face Hub.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Load the tokenizer.\n",
    "2. Define a `tokenize_function`.\n",
    "3. Use `.map()` to apply it to every example in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f172e-83fe-468e-afda-ddbcd4b545d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-uncased\"  # or any other model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    # return tokenizer(\n",
    "    #     example[\"text\"],\n",
    "    #     padding=\"max_length\",        # or \"longest\" / use DataCollator for dynamic padding\n",
    "    #     truncation=True,\n",
    "    #     max_length=128,\n",
    "    # )\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"longest\",        # or \"longest\" / use DataCollator for dynamic padding\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function, \n",
    "                                    batched=True)\n",
    "\n",
    "tokenized_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c329cb-9720-49e3-b7d5-eead87b32689",
   "metadata": {},
   "source": [
    "You should now see additional fields like:\n",
    "\n",
    "- `input_ids`\n",
    "- `token_type_ids` (for some models)\n",
    "- `attention_mask`\n",
    "\n",
    "These are what the model actually consumes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c61f3a-a080-4842-8d12-7846e59a4066",
   "metadata": {},
   "source": [
    "### 3. (Optional) Train / Evaluation Split\n",
    "\n",
    "`Trainer` usually expects separate **train** and **eval** datasets.\n",
    "\n",
    "We’ll just split our tiny dataset into 80% train, 20% validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e6d8aa-78dd-40eb-b66a-77c3536e53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce50788-7e84-4d58-a67b-64ee143678c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple train/test split\n",
    "splits = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = splits[\"train\"]\n",
    "eval_dataset = splits[\"test\"]\n",
    "\n",
    "len(train_dataset), len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354d683-6eee-4085-ab66-b932bb643586",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de070060-cc7a-487a-965f-8b8d6a18af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_dataset[0].items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0408355f-e46c-43b5-be88-358c01a28a9b",
   "metadata": {},
   "source": [
    "### 4. Define the Model and Training Arguments\n",
    "\n",
    "For classification we’ll use `AutoModelForSequenceClassification`.\n",
    "\n",
    "Key pieces:\n",
    "\n",
    "- `num_labels`: number of classes.\n",
    "- `TrainingArguments`: hyperparameters & output folder.\n",
    "  * [Doc for version currently on the JupyterHub](https://huggingface.co/docs/transformers/v4.57.5/en/main_classes/trainer#transformers.TrainingArguments)\n",
    "- `Trainer`: wraps the model, data, and training loop.\n",
    "  * [Doc for version currently on the JupyterHub](https://huggingface.co/docs/transformers/v4.57.5/en/main_classes/trainer#transformers.Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e79ee-4fcc-4333-9ae3-bb7d09155daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c829d5-880c-4b3b-90b8-eaff9d6af5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78117eb1-4357-49dc-8351-c0c1900ceda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=num_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079954df-1e29-46e5-8134-c7ad705d99d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-base-uncased-finetuned-sentiment\",\n",
    "    eval_strategy=\"epoch\",      # run eval at end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcebf523-b9be-40da-9ba0-86e88a278855",
   "metadata": {},
   "source": [
    "### 5. Define Metrics and Create the `Trainer`\n",
    "\n",
    "We’ll compute simple **accuracy** (you can add F1, precision, recall, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d8a18-c127-4b47-9d84-0b44a5fce3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "f1_metric = evaluate.load(\"f1\")  # or any other metric name\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return f1_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b07fbd-5bac-4a50-b889-b63c31f6abe1",
   "metadata": {},
   "source": [
    "### 6. Train the Model\n",
    "\n",
    "Now we can call `.train()` and let `Trainer` handle the rest.\n",
    "\n",
    "This will actually fine-tune the model on our tiny example dataset - in a real project you'd have thousands of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4bba70-3851-423a-bade-f1d612c699f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77cfe2-2f8d-48d6-b885-149ee4f46784",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3319c4-59bd-4910-a088-4f1ff09d566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651d57e2-cfa1-41e1-b614-548a2541731e",
   "metadata": {},
   "source": [
    "To feed your text into `transformers.Trainer`, you typically:\n",
    "\n",
    "1. Store your raw data in a `Dataset`:\n",
    "   - `Dataset.from_dict(...)` or `load_dataset(\"csv\", ...)`.\n",
    "2. Tokenize with a Hugging Face tokenizer:\n",
    "   - Use `.map(tokenize_function, batched=True)`.\n",
    "3. Split into `train_dataset` and `eval_dataset`.\n",
    "4. Instantiate a model (`AutoModelForSequenceClassification` or task-specific variant).\n",
    "5. Set up `TrainingArguments` and a `Trainer`.\n",
    "6. Call `.train()` and `.evaluate()`.\n",
    "\n",
    "You can plug your own text/labels into the same pipeline, and swap the model checkpoint to match your task (e.g. RoBERTa, DistilBERT, domain-specific models, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
