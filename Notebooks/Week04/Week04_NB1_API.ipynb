{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90524d64-47ac-4803-b6c8-036fef505b3c",
   "metadata": {},
   "source": [
    "# Using prompts programmatically\n",
    "\n",
    "We'll start looking at using prompts with APIs, and specifically with the OpenAI Python API library.\n",
    "* This is what we use with `from openai import OpenAI`\n",
    "  * https://github.com/openai/openai-python\n",
    "* AIMLAPI is a separate developer-focused platform providing API access to models, but it is compatible with the OpenAI SDK and provides pretty docs for models' API calls\n",
    "  * https://docs.aimlapi.com\n",
    "\n",
    "At the API level, using an LLM is basically:\n",
    "* Send a prompt (plus model + parameters)\n",
    "* Get back a response object\n",
    "* Pull out the text you care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31618afc-afc6-4c79-b7df-f9dda5e9f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import this for some better output printing\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e09807-a1fa-4fd3-9ec8-5698b6bc7902",
   "metadata": {},
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110bfc5e-d452-41a3-92b8-4c00fceb1fb3",
   "metadata": {},
   "source": [
    "Initialize the client object that we'll use to make API calls.\n",
    "* You should import your own API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d715bb-e71f-4935-995e-bb07ad96409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import keys\n",
    "client = OpenAI(api_key = keys.OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ab3942-4675-4d2f-b1df-4140211af9e4",
   "metadata": {},
   "source": [
    "The primary API for interacting with OpenAI models is the Responses API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98434ca1-8b39-483e-b5fa-dd6f194c38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",        # pick a model\n",
    "    input=\"Explain Python list comprehensions in 3 sentences.\"\n",
    ")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615b0fb-c85a-4f6a-8e30-5858fe48e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c65aa-4b28-4bcc-aa57-082969cb37b3",
   "metadata": {},
   "source": [
    "While `responses` is the recommended API approach.  There is an older approach with `chat.completions` that we will use instead.  \n",
    "* Rather than stack up API usage with OpenAI models, we'll use an API gateway through the platform supporting the class JupyterHub.\n",
    "* this API endpoint does not support `responses` **yet**\n",
    "* this will suit just fine, though in the near future (or if you use OpenAI for other work), you should be prepared to switch over to `responses`\n",
    "\n",
    "Key differences for us are:\n",
    "* use `client.chat.completions.create` rather than `client.responses.create`\n",
    "* different input arguments and output\n",
    "  * `messages` rather than `input`\n",
    "    * `messages` must strictly be an array of objects\n",
    "  * `max_tokens` rather than `max_output_tokens`\n",
    "  * `response.choices[0].message.content` rather than `response.output_text`\n",
    "* consult the API docs for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c2355-0671-4e57-9dd3-6f640e045689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent approach:\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5-nano\",        # pick a model\n",
    "    messages=[\n",
    "        {'role': 'user', \n",
    "         'content': 'Explain Python list comprehensions in 3 sentences.'}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1727a7-8307-42c6-9e43-d1ad4388223f",
   "metadata": {},
   "source": [
    "# One more example of responses vs chat completions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5228d6-d0bd-4e23-ab9f-739374abfc67",
   "metadata": {},
   "source": [
    "We can also use the list of dict as a value for `input` in the call to the `responses` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2b098-868e-4c50-9665-99020604686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent approach:\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",        # pick a model\n",
    "    input=[\n",
    "        {'role': 'user', \n",
    "         'content': 'Explain Python list comprehensions in 3 sentences.'}\n",
    "    ]\n",
    ")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74d7b92-038a-4d64-985d-6371b9ede3c3",
   "metadata": {},
   "source": [
    "# Flipping over to a different API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a14c96-489e-4fc8-88ec-d749190e6c06",
   "metadata": {},
   "source": [
    "Functionally we can work with our different endpoint very similarly. We will simply be using a different URL to run a different model, using a different token for authentication.\n",
    "\n",
    "We are using this as a courtesy of the National Research Platform.  I have loaded an API token for use directly into your environment variables.  Please be mindful and considerate with your use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f7fb8-55ee-4875-aeae-c89b805d21b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "NRP_TOK = os.environ.get('NRP_TOK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfca613-2781-4eff-8745-a3ac04c5d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using different API token and different API endpoint\n",
    "client = OpenAI(api_key = NRP_TOK,\n",
    "                base_url = \"https://ellm.nrp-nautilus.io/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7dff2-87fb-41aa-b70a-6f791169a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrp_hosted_model = 'gpt-oss'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed558d79-3bdb-4eaa-862d-064d14901d64",
   "metadata": {},
   "source": [
    "# Pieces\n",
    "\n",
    "* `model` – which model to use.\n",
    "* `messages` – a list of role-tagged messages.\n",
    "* Optional knobs: `max_tokens`, `temperature`, `top_p`, `top_k`\n",
    "  * note that if we were using `client.responses.create`, we would switch `messages` -> `input` and `max_tokens` -> `max_output_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d4096-e63e-4339-b8a4-66a25bff7cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=nrp_hosted_model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a friendly Python tutor. Explain things simply.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is a generator in Python? One short example, please.\"\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=256,          # cap length\n",
    "    temperature=0.3,         # more deterministic\n",
    "    top_p=1.0,               # consider full distribution\n",
    ")\n",
    "\n",
    "text = response.choices[0].message.content\n",
    "display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d1e8b-0f9a-48d7-8fbe-5281cdc1a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a52e6-27d0-47f2-a1f9-a069d26bc570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def givemecoffee(temp=1.0):\n",
    "    response = client.chat.completions.create(\n",
    "        model=nrp_hosted_model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Give me 5 weird startup ideas about coffee.\"\n",
    "            },\n",
    "        ],\n",
    "        max_tokens=256,\n",
    "        temperature=temp,\n",
    "    )\n",
    "    text = response.choices[0].message.content\n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0252f4e0-fc8c-464b-9bfe-8f9a580a1258",
   "metadata": {},
   "outputs": [],
   "source": [
    "givemecoffee(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41adc5b1-5d93-4694-b0da-2ff1150c4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "givemecoffee(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c59ee-11bf-446b-aac8-d84282baf37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "givemecoffee(1.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ec8a0c-87ee-4b9a-b29e-053e3c93347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "givemecoffee(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96086d6e-197a-46b0-9d1f-ffebee8fde0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "givemecoffee(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f89431-05f4-407c-82be-0b3099939189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def givemeCOLDcoffee(top_p=1.0):\n",
    "    response = client.chat.completions.create(\n",
    "        model=nrp_hosted_model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Give me 5 weird startup ideas about coffee.\"\n",
    "            },\n",
    "        ],\n",
    "        max_tokens=256,\n",
    "        temperature=0.1,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    text = response.choices[0].message.content\n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95305c7b-b3e5-450d-af23-9025de444e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "givemeCOLDcoffee(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926fa52d-a4c0-4e20-af5b-06d98d47e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "givemeCOLDcoffee(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d8dd1-0066-419e-a594-5af50d10d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "givemeCOLDcoffee(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9f7fe4-2321-4303-8d43-cdc037524145",
   "metadata": {},
   "outputs": [],
   "source": [
    "givemeCOLDcoffee(0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
