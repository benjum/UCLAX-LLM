{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69fe902f-6267-4aa7-9761-aed9cbbbb33d",
   "metadata": {},
   "source": [
    "# Running this with LOCAL models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11cd340-7ced-4159-8e16-d659ab97d896",
   "metadata": {},
   "source": [
    "Now let's look at passing prompts into a local model, rather than some model that gets run on an external service.\n",
    "\n",
    "We'll use the `transformers` library and two approaches:\n",
    "* using the text generation `pipeline` object\n",
    "* directly using a model via `AutoModelForCausalLM`\n",
    "\n",
    "We can exert more control with these, but it also requires that we get a bit more into the details of what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c404206-0821-45a0-93fa-e050aa1bb0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836861f-9e9e-4b59-84c2-7c83a65fa18d",
   "metadata": {},
   "source": [
    "We choose a model available on Hugging Face, but this can be swapped with other text generation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c9530-8d21-4579-9dd6-34dd212eded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c17771-cafc-4f12-b733-b6fbe1c14746",
   "metadata": {},
   "source": [
    "# transformers and pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de04d48b-78dc-4532-8816-233f3bb7b28e",
   "metadata": {},
   "source": [
    "Initialize our text generation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14970d6-f248-443a-811c-ce58f384f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\",\n",
    "                     model = model_name,\n",
    "                     tokenizer = model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff310b6-6cef-4196-9dcc-f9af6f3aee9c",
   "metadata": {},
   "source": [
    "Initialize our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf975e-5381-4d69-a254-29d4ffa0da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\"Explain what a neural network is in one sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc32d8f6-ad84-4027-8ad2-86bd6fa21bde",
   "metadata": {},
   "source": [
    "Get the output from our pipeline for this prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b2b6a-bc2f-4643-bcc3-8d59027b13f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generator(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc795b-a20c-4c8f-8da0-badaed50b4a0",
   "metadata": {},
   "source": [
    "What output do we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff64cc-471f-4b5a-a66a-e13c0a8d7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cec3a6-27b9-4d45-9cb5-c91da5c9f4e2",
   "metadata": {},
   "source": [
    "We have to exercise more care here than with the API calls.  \n",
    "\n",
    "For example, the prompt itself is included in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a4cc1-aa66-4b0d-92e6-d1f7472f92b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dd0a91-71fe-40d8-87a3-2b57a354165c",
   "metadata": {},
   "source": [
    "We can start customizing and adapting the LLM by dialing the knobs like:\n",
    "* temperature\n",
    "* top_p\n",
    "* max number of output tokens (max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a8f177-e295-4fe7-bad6-7ab8ccedac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generator(prompt,\n",
    "                    max_new_tokens=64)\n",
    "\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37045c16-6ae6-4994-81e5-097927fbae63",
   "metadata": {},
   "source": [
    "Beware that you may need to set `do_sample=True` in order for the text generation pipeline to work with variable temperature.  For our model that is not the case, but we set it anyways to make this generalizable across other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab9867-6da1-4685-aeb5-c8da763995bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generator(prompt,\n",
    "                    max_new_tokens=64,\n",
    "                    do_sample=True,\n",
    "                    temperature=2.0)\n",
    "\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a025bf-c0f1-4d2b-8cf1-5e082a0c913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generator(prompt,\n",
    "                    max_new_tokens=64,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.1)\n",
    "\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5479673-b7ba-4c3d-8b2f-8e99d115fdd7",
   "metadata": {},
   "source": [
    "Bring in top_k too (for limiting the number of possible tokens to select from when predicting a next-token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaf7504-94f7-49ac-a3f3-046baf23d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generator(prompt,\n",
    "                    max_new_tokens=64,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.1,\n",
    "                    top_k=10)\n",
    "\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1aadb-430b-4647-a02a-3015ecf907bd",
   "metadata": {},
   "source": [
    "We can also start expanding the elements in our prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee18a6-8b93-4186-b40d-dd90b576bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"You are a helpful assistant.\\n\"\n",
    "    \"User: Explain what a neural network is in one sentence.\\n\"\n",
    "    \"Assistant:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eda371-089b-446a-bdd0-035b5d7d8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generator(prompt,\n",
    "                    max_new_tokens=64,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4486bd-0c18-4040-a985-0d771a13fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[0][\"generated_text\"][len(prompt):])\n",
    "# print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b287c-f16b-4417-9e82-7b03ba410112",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generator(prompt,\n",
    "                    max_new_tokens=64,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.1,\n",
    "                    top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac9b0c-eb80-40c3-a18b-d1c8420ddac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7dae85-69c3-4bec-a71b-51ad2c726a3e",
   "metadata": {},
   "source": [
    "# Running models more explicitly (without pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e325e-47d0-49bb-8f78-01fd54d39632",
   "metadata": {},
   "source": [
    "We can be more hands-on when running models, though we need to be even more explicit in what steps are occurring.\n",
    "* tokenization\n",
    "* generation\n",
    "* conversion from tokens to output string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a3b7fa-7a46-45f3-8b46-164f054cb836",
   "metadata": {},
   "source": [
    "Initialize our model and tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c726afc-6097-46e5-ac2f-0351466c2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0aecb0-fab1-44c3-b241-ba77a38f8bcb",
   "metadata": {},
   "source": [
    "Slight (optional) customization of our model and tokenizer for padding tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f22dd8-5442-47c1-9596-8cc779636a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abc1bd0-ffe6-463f-bd1b-844e35af8bc9",
   "metadata": {},
   "source": [
    "Specify that we run our model on a GPU if available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42605d-cbd3-40c6-b1d4-6a9c6d8d35d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8e1bb-5991-4a5b-ab50-65c48c25f3bf",
   "metadata": {},
   "source": [
    "Initialize our prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0bb4fa-6735-4b38-9101-9e6bfa8278b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"You are a helpful assistant.\\n\"\n",
    "    \"User: Explain what a neural network is in one sentence.\\n\"\n",
    "    \"Assistant:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d587c-6e67-4e2b-be55-f2fb6646e3be",
   "metadata": {},
   "source": [
    "We need to tokenize the prompt and move to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff0507-abaa-482d-b885-f413eebbbdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9245805-2527-4830-906d-4379f8acacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc8132-2215-4ffa-a81b-dacbb3dd4e95",
   "metadata": {},
   "source": [
    "Get the output from our model for this prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839490a-4e4d-4b1f-b1bb-78c77b0f6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f15100-44cc-4c27-a844-1e0f530d5bb7",
   "metadata": {},
   "source": [
    "The model output is in token form, inside a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e5ccd-ba50-42dc-81d2-53620e09bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502ce6f-e318-4f3c-a010-e7ef65c5ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca0195b-9014-444c-b525-718df1f02d90",
   "metadata": {},
   "source": [
    "It also includes the input prompt, which we can slice out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8317d3a6-47c2-4a86-8c48-b82c385bbcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids[0][len(inputs['input_ids'][0]):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad8bab-79f7-423e-a83e-740d53f6797e",
   "metadata": {},
   "source": [
    "To get a human-readable form, we can pass this into the tokenizer again, this time to decode the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2cc141-8503-4458-81f0-7dcc3ccc1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.decode(output_ids[0][len(inputs['input_ids'][0]):],\n",
    "                            skip_special_tokens=False)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b66b2cf-0370-4961-853a-e82389678dd0",
   "metadata": {},
   "source": [
    "# system and user roles\n",
    "\n",
    "The basic technique above can be expanded in many ways.\n",
    "\n",
    "One essential way is to use different roles for adapting the LLM behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0864a09-2a58-4e0a-9359-ea54f08eef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful, concise AI assistant.\"\n",
    "user_prompt = \"Explain what overfitting is in machine learning.\"\n",
    "\n",
    "# Easiest: just stitch them together into one prompt string\n",
    "full_prompt = (\n",
    "    f\"{system_prompt}\\n\\n\"\n",
    "    f\"User: {user_prompt}\\n\"\n",
    "    f\"Assistant:\"\n",
    ")\n",
    "\n",
    "full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08460e78-cb46-40fb-9b42-40e8f5e361c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful, concise AI assistant.\"\n",
    "user_prompt = \"Explain what overfitting is in machine learning.\"\n",
    "\n",
    "# Easiest: just stitch them together into one prompt string\n",
    "full_prompt = (\n",
    "    f\"{system_prompt}\\n\\n\"\n",
    "    f\"User: {user_prompt}\\n\"\n",
    "    f\"Assistant:\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "response_text = tokenizer.decode(output_ids[0][len(inputs['input_ids'][0]):], \n",
    "                                 skip_special_tokens=True)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f13954-29b1-42e8-a864-be247983c12d",
   "metadata": {},
   "source": [
    "# Using chat template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74c27f-16e9-40ce-a608-a608e3c5f32b",
   "metadata": {},
   "source": [
    "Similarly to the API use, we can specify a list of dicts to specify roles and the prompt content specific to the different roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c138d4b-6729-41ed-b525-b1b51c21f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful, concise AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain what overfitting is in machine learning.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a9333-963b-4c40-b3fc-1a81f7147369",
   "metadata": {},
   "source": [
    "If we do this, however, we need to make sure the list gets translated into an appropriate syntax for whichever model we're using.\n",
    "\n",
    "The `tokenizer` is specific to our model, and we can use its methods to do this translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424270d-18a4-4aa5-bd04-8e8f1dc7a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,           # return a plain string, not token IDs\n",
    "    add_generation_prompt=True,  # append the assistant turn marker\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9be6e2-6185-454f-9647-e4392ce83a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,           # return a plain string, not token IDs\n",
    "    add_generation_prompt=True,  # append the assistant turn marker\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3706f55-d984-465b-b0e9-dce74d9c28f6",
   "metadata": {},
   "source": [
    "The ouput of `tokenizer.apply_chat_template(messages,...)` crafts our prompt into the form that now has the correct syntax to feed into our call to the model.\n",
    "\n",
    "That said, we still need to be careful that the prompt is tokenized and sent to the GPU before passing it into the `generate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12dead3-d549-48a2-8fe3-5b75424c951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful, concise AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain what overfitting is in machine learning.\"},\n",
    "]\n",
    "\n",
    "# Build a single text prompt from messages\n",
    "full_prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,  # adds the assistant turn marker\n",
    ")\n",
    "\n",
    "inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "response_text = tokenizer.decode(output_ids[0][len(inputs['input_ids'][0]):], \n",
    "                                 skip_special_tokens=True)\n",
    "print(response_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2a32e-cd45-4dd4-8a4a-493d13dbadbf",
   "metadata": {},
   "source": [
    "# Connecting back to pipeline and API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819f670b-33d7-49ac-ae60-19bd2d6e7d3a",
   "metadata": {},
   "source": [
    "We can use the roles similarly with the text generation pipeline.\n",
    "* We can use the prompt directly as a single string.\n",
    "* We can also use the list of roles/contents, provided we conform it with the apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e8563-2502-4741-9e26-63b757484fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\",\n",
    "                     model=model_name,\n",
    "                     tokenizer=model_name)\n",
    "\n",
    "system_prompt = \"You are a helpful, concise AI assistant.\"\n",
    "user_prompt = \"Give me a short explanation of gradient descent.\"\n",
    "\n",
    "prompt = (\n",
    "    f\"{system_prompt}\\n\\n\"\n",
    "    f\"User: {user_prompt}\\n\"\n",
    "    f\"Assistant:\"\n",
    ")\n",
    "\n",
    "outputs = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043e443e-f6c7-46ab-8d1c-a8abbcf9bbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\",\n",
    "                     model=model_name,\n",
    "                     tokenizer=model_name)\n",
    "\n",
    "system_prompt = \"You are a helpful, concise AI assistant.\"\n",
    "user_prompt = \"Give me a short explanation of gradient descent.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "outputs = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65edcee3-925b-41aa-b2a9-35d146b1e851",
   "metadata": {},
   "source": [
    "### API\n",
    "\n",
    "The API is even more bare, in that we don't need to worry about the tokenization/de-tokenization or chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a1e2a8-ef82-4cb9-8239-400f81891704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "NRP_TOK = os.environ.get('NRP_TOK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e4b83e-0086-41bb-915d-1a60b8f13d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key = NRP_TOK,\n",
    "                base_url = \"https://ellm.nrp-nautilus.io/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a5557a-689a-44f4-a8b0-7f841c34e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful, concise AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a one-sentence summary of the Big Bang theory.\"},\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-oss',   # or another chat model you have access to\n",
    "    messages=messages,\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bd2cfa-3799-4047-863f-c4b081588145",
   "metadata": {},
   "source": [
    "The server handles:\n",
    "\n",
    "* combining those messages into a single internal prompt,\n",
    "* adding whatever special tokens / separators / role markers it uses,\n",
    "* tokenizing that,\n",
    "* running the model,\n",
    "* and then detokenizing the output back to text.\n",
    "\n",
    "You never see the equivalent of apply_chat_template or the exact raw prompt string; thatâ€™s all abstracted away."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
