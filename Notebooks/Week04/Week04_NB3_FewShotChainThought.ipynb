{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da4887f-c441-46ec-9f0c-c122e71d51cf",
   "metadata": {},
   "source": [
    "# Few-Shot and Chain-of-Thought Prompting\n",
    "\n",
    "Let's consider some examples of how model output changes depending on whether we prompt by:\n",
    "* Providing examples (zero-shot vs few-shot)\n",
    "* Telling the model to reason, e.g. step-by-step for chain-of-thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24d592-db13-46c0-bb72-1108b6fea60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI \n",
    "NRP_TOK = os.environ.get('NRP_TOK')\n",
    "client = OpenAI(api_key = NRP_TOK, base_url = \"https://ellm.nrp-nautilus.io/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd976996-c56d-4783-aad1-375e396be6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL='gpt-oss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16320f8-214e-4487-b18d-d85c8ce7a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(user_prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d74f6e3-d121-4dd7-97dc-0815645a6a7f",
   "metadata": {},
   "source": [
    "## Zero-shot vs Few-shot\n",
    "\n",
    "Zero-shot prompting involves giving a task to a model without any examples, relying solely on its pre-trained knowledge to generate a response. \n",
    "\n",
    "Few-shot prompting enhances performance by including a small number of examples in the prompt, helping the model understand the task's pattern or format. \n",
    "\n",
    "While zero-shot is simpler and faster, few-shot typically yields more accurate and consistent results by providing contextual guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4824c5c2-7f61-4edf-8384-bf47924e095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_1 = '''\n",
    "Classify the sentiment of the review.\n",
    "\n",
    "Review:\n",
    "{review}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45bc9b4-b684-4369-b4d9-b07562540874",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = [\n",
    "    'you probably like this movie, but i did not.',\n",
    "    'loved it',\n",
    "    'it made my eyes glaze over',\n",
    "    'it made me teary eyed'\n",
    "]\n",
    "for i in review_text:\n",
    "    print(i)\n",
    "    review_prompt = prompt_template_1.format(review=i)\n",
    "    result = call_llm(review_prompt)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef8dd80-5316-4dbe-83b3-1b2a059c46c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_2 = '''\n",
    "Classify the sentiment of the review as \"positive\", \"neutral\", or \"negative\".\n",
    "\n",
    "Review:\n",
    "{review}\n",
    "\n",
    "Answer with one word: positive, neutral, or negative.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d40e6d1-8319-4ed4-a4e0-167df58542a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = [\n",
    "    'you probably like this movie, but i did not.',\n",
    "    'i loved it intensely',\n",
    "    'it made my eyes glaze over',\n",
    "    'it made me teary eyed'\n",
    "]\n",
    "for i in review_text:\n",
    "    print(i)\n",
    "    review_prompt = prompt_template_2.format(review=i)\n",
    "    result = call_llm(review_prompt)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539604f7-0323-46c4-8465-edb3fc19c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_3 = '''\n",
    "Classify the sentiment of the review as \"positive\", \"neutral\", or \"negative\".\n",
    "\n",
    "Example:\n",
    "Review: 'you probably like this movie, but i did not.'\n",
    "Sentiment: negative\n",
    "\n",
    "Example:\n",
    "Review: 'i loved it intensely'\n",
    "Sentiment: positive\n",
    "\n",
    "Example:\n",
    "Review: 'it made me teary eyed'\n",
    "Sentiment: neutral\n",
    "\n",
    "Output:\n",
    "Answer with one word: positive, neutral, or negative.\n",
    "\n",
    "Review:\n",
    "{review}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bbe758-792f-4d00-a9f4-23aa8b318c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = [\n",
    "    'you probably like this movie, but i did not.',\n",
    "    'i loved it intensely',\n",
    "    'it made my eyes glaze over',\n",
    "    'it made me teary eyed',\n",
    "    'i cried'\n",
    "]\n",
    "for i in review_text:\n",
    "    print(i)\n",
    "    review_prompt = prompt_template_3.format(review=i)\n",
    "    result = call_llm(review_prompt)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b6929-04c1-4d8e-902e-b16785111691",
   "metadata": {},
   "source": [
    "## Chain-of-Thought\n",
    "\n",
    "Chain-of-thought prompting enhances LLM reasoning by guiding them to break down complex problems into smaller, logical steps before arriving at a final answer.  It works best with larger models and is particularly effective when the model is prompted with examples or clear instructions to think step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7fc75-7b28-4825-94bb-cfd6cf0d1c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_4 = '''\n",
    "You are a movie critic. Classify the sentiment of the review \n",
    "as \"positive\", \"neutral\", or \"negative\". Show your reasoning\n",
    "step-by-step.\n",
    "\n",
    "Review:\n",
    "{review}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bbe0aa-c228-4413-9314-be2b44d85612",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = [\n",
    "    'you probably like this movie, but i did not.',\n",
    "    'i loved it intensely',\n",
    "    'it made my eyes glaze over',\n",
    "    'it made me teary eyed',\n",
    "    'i cried'\n",
    "]\n",
    "for i in review_text:\n",
    "    print(i)\n",
    "    review_prompt = prompt_template_4.format(review=i)\n",
    "    result = call_llm(review_prompt)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff56257-c3ab-4f09-8675-1fd473b266c3",
   "metadata": {},
   "source": [
    "This method can be especially useful for:\n",
    "* Word problems and logic puzzles\n",
    "* Complex instructions with many constraints (“Write code that does X, Y, Z but not W”)\n",
    "* Planning problems (trip plans, project plans, multi-step workflows)\n",
    "* Refactoring or debugging code (explain issues → then propose fix)\n",
    "\n",
    "The technique can help to:\n",
    "* Reduce the probability of \"magic answers\".\n",
    "* Improve accuracy on multi-step problems (math, logic, planning, code, multi-constraint instructions).\n",
    "* Make debugging easier: If the model is wrong, you can see where reasoning went off the rails.\n",
    "* Educate: you see intermediate steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d5a324-0703-4678-ba01-fa6fda4f9808",
   "metadata": {},
   "outputs": [],
   "source": [
    "life_planner = '''\n",
    "You are a life coach and college counselor.\n",
    "\n",
    "Task:\n",
    "Help an undergraduate student choose between two career tracks.\n",
    "\n",
    "Steps:\n",
    "1. List pros and cons of approach A.\n",
    "2. List pros and cons of approach B.\n",
    "3. State your recommendation and explain in 2–3 sentences.\n",
    "\n",
    "Approach A:\n",
    "{track1}\n",
    "\n",
    "Approach B:\n",
    "{track2}\n",
    "\n",
    "Format:\n",
    "- Plain text.\n",
    "- No Markdown.\n",
    "\n",
    "Answer:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7a606-b34d-4cb9-9a32-dbed094e2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_prompt = life_planner.format(track1='novelist',\n",
    "                                    track2='AI engineer')\n",
    "result = call_llm(tracks_prompt)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
