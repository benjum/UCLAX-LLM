{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "922b0bcb",
   "metadata": {},
   "source": [
    "# Bringing in LangChain for our Prompt Engineering\n",
    "\n",
    "After gaining experience and skill with using single prompts and API use, we can start building this out even more by building prompt pipelines.\n",
    "\n",
    "We'll move from single prompts to prompt pipelines using LangChain (in tandam with our API gateway that uses the Chat Completions API).\n",
    "\n",
    "By the end, we'll:\n",
    "- Call an LLM via LangChain\n",
    "- Turn a raw prompt into a reusable prompt template\n",
    "- Build a simple chain: `prompt -> model -> output parser`\n",
    "- Build a small two-step pipeline: `reasoning -> short final answer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7d4757",
   "metadata": {},
   "source": [
    "## Initialize our LangChain object\n",
    "\n",
    "In this notebook we use LangChain `ChatOpenAI`, but we point it at a gateway that speaks the OpenAI style `chat.completions` API.\n",
    "\n",
    "Key idea:\n",
    "- the gateway exposes a `POST /v1/chat/completions` endpoint\n",
    "- we pass `base_url` and `api_key` for that gateway\n",
    "- we set `use_responses_api=False` so LangChain uses chat.completions instead of the newer responses API\n",
    "\n",
    "You can adapt the base URL and model name to match other setups as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683460dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import os\n",
    "\n",
    "NRP_TOK = os.environ.get('NRP_TOK')\n",
    "nrp_llm_url = \"https://ellm.nrp-nautilus.io/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model = 'gpt-oss',\n",
    "                 api_key = NRP_TOK,\n",
    "                 base_url = nrp_llm_url,\n",
    "                 # use_responses_api=False forces the classic chat.completions endpoint\n",
    "                 use_responses_api=False,\n",
    "                 temperature = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c39c6",
   "metadata": {},
   "source": [
    "## From single prompts to reusable building blocks\n",
    "\n",
    "Prompt engineering usually starts with one off strings such as `Explain X in a friendly way`.\n",
    "\n",
    "But as soon as we want to:\n",
    "- reuse the same style with different inputs\n",
    "- add multiple steps, for example draft -> critique -> summarise\n",
    "- mix in tools or retrieval\n",
    "\n",
    "we end up needing more structure.\n",
    "\n",
    "LangChain gives us:\n",
    "- models: `ChatOpenAI`\n",
    "- prompt templates: parameterised prompts with variables\n",
    "- chains: composable flows, for example `prompt -> model -> parser -> next step`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2819afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: directly pass a single prompt string to the model.\n",
    "\n",
    "response = llm.invoke('In 2â€“3 sentences, what is LangChain, in simple terms?')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3922fc26",
   "metadata": {},
   "source": [
    "## Prompt templates\n",
    "\n",
    "Instead of hard coding every prompt as a plain string, we can define a template once and fill in variables.\n",
    "\n",
    "Benefits for prompt engineers:\n",
    "- reuse: same structure, different inputs\n",
    "- experimentation: tweak one template, not many copies\n",
    "- separation of concerns: prompt design versus application logic\n",
    "\n",
    "We will use `ChatPromptTemplate`, which is tailored for chat style prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea48c7-bc41-4e15-b4e6-5d109a4f7d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a reusable prompt template for explanations.\n",
    "\n",
    "explain_prompt = ChatPromptTemplate.from_template(\n",
    "    '''\n",
    "    You are a helpful AI tutor.\n",
    "\n",
    "    Explain the concept of {topic} to a beginner\n",
    "    in a {tone} tone, for example friendly, formal, or playful.\n",
    "    Use short paragraphs and concrete examples.\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81f2c9-4284-4ff3-84de-78e091ada88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c877969-3c88-4e99-9536-4b414e947dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd09af-879f-4002-a841-b872842ab62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_prompt.invoke({'topic': 'LangChain', 'tone': 'friendly'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5904e01e-8061-4c43-ae49-1d1748232c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the model response as a simple string.\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b970e895-7764-4602-adb8-1f3736f88102",
   "metadata": {},
   "source": [
    "Building our chain: prompt -> model -> output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce30320-a5e4-4661-a624-3c13bd22f944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explain_chain = explain_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d160437-c970-4ca0-88ed-d80a47ca7757",
   "metadata": {},
   "source": [
    "**Friendly** explanation of vector databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973017d3-70aa-4d5c-957e-dbb526378677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = explain_chain.invoke(\n",
    "    {'topic': 'vector databases', \n",
    "     'tone': 'friendly'}\n",
    ")\n",
    "\n",
    "display(Markdown(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c9c29-cc70-412f-94dd-9c45f650f775",
   "metadata": {},
   "source": [
    "**Formal** explanation of vector databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54bfab4-88a1-4a1f-82ba-b8669b186752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = explain_chain.invoke(\n",
    "    {'topic': 'vector databases', \n",
    "     'tone': 'formal'}\n",
    ")\n",
    "\n",
    "display(Markdown(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f660bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## From single chain to a tiny pipeline\n",
    "\n",
    "Prompt engineering often benefits from staging:\n",
    "1. let the model reason in detail\n",
    "2. then summarise that reasoning for the user\n",
    "\n",
    "We will build:\n",
    "- a first chain that generates detailed step by step reasoning\n",
    "- a second step that compresses that into a short, user friendly answer\n",
    "\n",
    "LangChain lets us compose these steps with the LangChain Expression Language using the `|` operator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36c740-ac0f-4321-b165-6f282b5813b6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Step 1: a prompt that asks the model to think step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ae761-84c2-4d76-b9db-55832a7533a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reasoning_prompt = ChatPromptTemplate.from_template(\n",
    "    '''\n",
    "    You are a careful reasoning assistant.\n",
    "\n",
    "    Think step by step to answer the user question.\n",
    "    Show your reasoning explicitly, with numbered steps.\n",
    "\n",
    "    Question: {question}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ca86e-c461-402a-85d0-70fdf028c389",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reasoning_chain = reasoning_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc5764e-3705-4e5c-8bc0-5d096866f14f",
   "metadata": {},
   "source": [
    "Test it out once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019a7056-2869-4763-80ad-278759623222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reasoning_example = reasoning_chain.invoke(\n",
    "    {'question': 'Why are large language models called large?'}\n",
    ")\n",
    "\n",
    "display(Markdown(reasoning_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6167ff91-0a5f-4113-8125-5d4bdd7e42fa",
   "metadata": {},
   "source": [
    "### Step 2: a prompt that summarises detailed reasoning into a short answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598c646-d928-4ed9-9200-573ae27ed9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_prompt = ChatPromptTemplate.from_template(\n",
    "    '''\n",
    "    You are a friendly assistant.\n",
    "\n",
    "    You are given a user question and some detailed reasoning.\n",
    "    Your task is to write a clear, concise answer for the user\n",
    "    in 2 to 3 sentences, without showing the intermediate steps.\n",
    "\n",
    "    Question: \n",
    "    {question}\n",
    "\n",
    "    Reasoning:\n",
    "    {reasoning}\n",
    "\n",
    "    Final answer, with no preamble, just the answer:\n",
    "    '''\n",
    ")\n",
    "\n",
    "summarize_chain = summarize_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c524e-4682-4652-afff-8e79b2f7c304",
   "metadata": {},
   "source": [
    "### Combine both steps into a single chain.\n",
    "* Input: a question string.\n",
    "* Internal:\n",
    "  * RunnablePassthrough carries the original question through\n",
    "  * reasoning_chain produces the reasoning text\n",
    "* Output: a short final answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a2340",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_step_chain = (\n",
    "    {\n",
    "        'question': RunnablePassthrough(),\n",
    "        'reasoning': reasoning_chain,\n",
    "    }\n",
    "    | summarize_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = 'Why do we use prompt templates instead of plain strings?'\n",
    "final_answer = two_step_chain.invoke(question)\n",
    "\n",
    "print('Question:', question)\n",
    "print('\\nFinal answer:\\n', final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d6b838",
   "metadata": {},
   "source": [
    "We have now:\n",
    "1. called a chat model via LangChain\n",
    "2. turned a raw prompt into a reusable `ChatPromptTemplate`\n",
    "3. built a simple chain: `prompt -> model -> parser`\n",
    "4. composed a two step pipeline: detailed reasoning then a short answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
